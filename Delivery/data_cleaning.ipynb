{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Even though we now have all our extracted data, there are still null values and string and object format columns. Because of that, we had to clean our data.\n",
    "\n",
    "We start by importing relevant libraries and the dataframes we obtained in the feature extraction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyradiomics = pd.read_csv('pyradiomics_features_fixed.csv', index_col=False)\n",
    "df_pylidc = pd.read_csv('pylidc_features_fixed.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "\n",
    "Before merging both dataframes, we decided to tackle this problem using a binary target, malign and benign.\n",
    "\n",
    "#### Default distribution\n",
    "\n",
    "We start by looking at how our target's distribution by the different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, counts = np.unique(df_pylidc.malignancy, return_counts=True)\n",
    "plt.bar(unique_labels, counts)\n",
    "plt.xlabel('Class label')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Class distribution in dataset')\n",
    "plt.show()\n",
    "\n",
    "print(unique_labels, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step for turning our target binary is merging similar classes:\n",
    "\n",
    "- -1  -> 'Highly Unlikely'(1) and 'Moderately Unlikely'(2)\n",
    "- 0  -> 'Indeterminate'(3)\n",
    "- 1  -> 'Moderately Suspicious'(4) and 'Highly Suspicious'(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_pylidc)):\n",
    "    if df_pylidc.at[i,'malignancy'] == 1 or df_pylidc.at[i,'malignancy'] == 2:\n",
    "        df_pylidc.at[i,'malignancy'] = -1\n",
    "    elif df_pylidc.at[i,'malignancy'] == 4 or df_pylidc.at[i,'malignancy'] == 5:\n",
    "        df_pylidc.at[i,'malignancy'] = 1\n",
    "    else:\n",
    "        df_pylidc.at[i,'malignancy'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, counts = np.unique(df_pylidc.malignancy, return_counts=True)\n",
    "plt.bar(unique_labels, counts)\n",
    "plt.xlabel('Class label')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Class distribution in dataset')\n",
    "plt.show()\n",
    "\n",
    "print(unique_labels, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform '0' labeled examples into the most common class within each patient. For that we start by defining a function that returns the most common class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_class(group):\n",
    "    mode_class = group['malignancy'].mode()\n",
    "    if not mode_class.empty:\n",
    "        return mode_class.iloc[0]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a dictionary to store the most common class for each patient and change our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_per_patient = df_pylidc.groupby('patient_id').apply(most_common_class).to_dict()\n",
    "\n",
    "df_pylidc['malignancy'] = df_pylidc.apply(lambda row: most_common_per_patient.get(row['patient_id'], row['malignancy']) if row['malignancy'] == 0 else row['malignancy'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, counts = np.unique(df_pylidc.malignancy, return_counts=True)\n",
    "plt.bar(unique_labels, counts)\n",
    "plt.xlabel('Class label')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Class distribution in dataset')\n",
    "plt.show()\n",
    "\n",
    "print(unique_labels, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we remove the remaining patients with target '0' as majority. For that, we group the dataframe by 'patient_id', count the ocurrences of each diagnosis and identify the ones we want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_counts = df_pylidc.groupby('patient_id')['malignancy'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "majority_0_patients = diagnosis_counts.idxmax(axis=1) == 0\n",
    "\n",
    "df_pylidc = df_pylidc[~df_pylidc['patient_id'].isin(majority_0_patients[majority_0_patients].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, counts = np.unique(df_pylidc.malignancy, return_counts=True)\n",
    "plt.bar(unique_labels, counts)\n",
    "plt.xlabel('Class label')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Class distribution in dataset')\n",
    "plt.show()\n",
    "\n",
    "print(unique_labels, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pylidc.to_csv('final_pylidc_extraction.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Merge\n",
    "\n",
    "The next step, after having the target in the format we wanted, is to merge both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pylidc.shape)\n",
    "print(df_pyradiomics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pyradiomics.merge(df_pylidc, left_on='id', right_on='Id')\n",
    "df = df.drop(columns=['Id'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns removal\n",
    "\n",
    "#### Null Values and Constants\n",
    "\n",
    "We removed all the columns that are constants in the dataframe or that contained null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = 0\n",
    "for col in df.columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        df = df.drop(columns=col)\n",
    "        removed += 1\n",
    "\n",
    "for col in df.columns:\n",
    "    null_count = df[col].isnull().sum()\n",
    "    if null_count > 0:\n",
    "        df = df.drop(columns=col)\n",
    "        removed += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were left with the following columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(f\"Total number of columns deleted: {removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took then a look at the number of individual patients in the training set and the resulting distribution of the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['patient_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, counts = np.unique(df.malignancy, return_counts=True)\n",
    "plt.bar(unique_labels, counts)\n",
    "plt.xlabel('Class label')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Class distribution in dataset')\n",
    "plt.show()\n",
    "\n",
    "print(unique_labels, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String and Object Types\n",
    "\n",
    "We need to be careful when working with string and object types, so we start by finding out how many we had, and which they were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby_dtypes = df.columns.to_series().groupby(df.dtypes).groups\n",
    "\n",
    "for key, value in df_groupby_dtypes.items():\n",
    "    if key == 'object':\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then gathered the object columns (except for ids) and removed them.\n",
    "\n",
    "Some contained coordinate tuples, which we below converted into new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = [\n",
    "    'diagnostics_Mask-original_Spacing',\n",
    "    'diagnostics_Mask-original_Size',\n",
    "    'diagnostics_Mask-original_BoundingBox',\n",
    "    'diagnostics_Mask-original_CenterOfMassIndex',\n",
    "    'diagnostics_Mask-original_CenterOfMass',\n",
    "    'diagnostics_Image-interpolated_Spacing',\n",
    "    'diagnostics_Image-interpolated_Size',\n",
    "    'diagnostics_Mask-interpolated_Spacing',\n",
    "    'diagnostics_Mask-interpolated_Size',\n",
    "    'diagnostics_Mask-interpolated_BoundingBox',\n",
    "    'diagnostics_Mask-interpolated_CenterOfMassIndex',\n",
    "    'diagnostics_Mask-interpolated_CenterOfMass',\n",
    "    'diagnostics_Mask-original_Hash',\n",
    "    'original_shape_Elongation', \n",
    "    'original_shape_Flatness',\n",
    "    'original_shape_LeastAxisLength', \n",
    "    'original_shape_MajorAxisLength',\n",
    "    'original_shape_MinorAxisLength',\n",
    "    ]\n",
    "\n",
    "object = df[object_columns]\n",
    "df = df.drop(columns = object_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To treat string type columns '(x, y, z)' we iterated and separated them into separate columns x, y and z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0, 12):\n",
    "    col = object[object_columns[j]]\n",
    "    comma_count = col.iloc[0].count(',')\n",
    "\n",
    "    # define new column names \n",
    "    list = []\n",
    "    for i in range(comma_count + 1):\n",
    "        name = object_columns[j] + \"_\" + str(i)\n",
    "        list.append(name)\n",
    "\n",
    "    for k in range(len(object)):\n",
    "        s = col.iloc[k]\n",
    "\n",
    "        # split the column into separate columns\n",
    "        s = s.strip('()')\n",
    "        values = [float(idx) for idx in s.split(', ')]\n",
    "        \n",
    "        for k1 in range(len(list)):\n",
    "            object[list[k1]] = values[k1]\n",
    "\n",
    "    object = object.drop(columns= object_columns[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we dropped the remaining columns with questionable data, such as hash values or imaginary values, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object = object.drop(columns= object.columns[0:6])\n",
    "object.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When merging the new columns with the dataframe, we can see we no longer have object types asides from 'id' and 'patient_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, object], axis=1)\n",
    "print(df.dtypes.value_counts())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Correlation\n",
    "\n",
    "We checked the correlation of our features with the target and decided to removed the ones with correlation lower than 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df[['id', 'patient_id']]\n",
    "features = df.drop(columns=['id', 'patient_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_with_target = features.corr()['malignancy']\n",
    "features_to_remove = correlations_with_target[abs(correlations_with_target) < 0.05].index\n",
    "df_filtered = features.drop(columns=features_to_remove)\n",
    "\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([index, df_filtered], axis=1) \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved our final CSV file, which will be used for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
