{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Statistical Analysis and Comparison\n",
    "\n",
    "To conclude our project, in this section we will discuss how we compare the performance of our models using a Wilcoxon Hypothesis Test. Our objective is to determine if the models are statistically different in terms of their performance on a given metric. \n",
    "\n",
    "We start by importing relevant libraries and our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfold_and_metrics import *\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final.csv\")\n",
    "df = df.drop(columns=[\"id\"])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be considering the following hyperparameters, taken from the hypertuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': 25,\n",
    "\t'max_depth': 25,\n",
    "\t'min_samples_leaf': 5,\n",
    "\t'criterion': \"log_loss\"\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': 25,\n",
    "\t'max_depth': 5,\n",
    "\t'min_child_weight': 15\n",
    "}\n",
    "\n",
    "nn_params = {\n",
    "    'hidden_layer_nodes': 60,\n",
    "\t'hidden_layer_activation': \"relu\",\n",
    "\t'learning_rate': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these, we compile the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(**rf_params)\n",
    "xgb = XGBClassifier(**xgb_params)\n",
    "\n",
    "nn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((100,), name=\"input\"),\n",
    "    tf.keras.layers.Dense(nn_params['hidden_layer_nodes'], activation=nn_params['hidden_layer_activation']),\n",
    "    tf.keras.layers.Dense(2,activation='softmax')\n",
    "])\n",
    "\n",
    "nn.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(nn_params['learning_rate']), \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrixes\n",
    "\n",
    "Taking a first look at the confusion matrixes, we are able to have a first idea of the models performances.\n",
    "\n",
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scores = k_fold_cv(model=rf, df=df, pca_components=100, show_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_scores = k_fold_cv(xgb, df, pca_components=100, show_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_scores = k_fold_cv_keras(compiled_model=nn, df=df, pca_components=100, show_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Metrics\n",
    "\n",
    "We also took a first look at the F1, Accuracy and AUC scores.\n",
    "\n",
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics = mean_std_results_k_fold_CV(rf_scores)\n",
    "rf_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_metrics = mean_std_results_k_fold_CV(xgb_scores)\n",
    "xgb_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics = mean_std_results_k_fold_CV(nn_scores)\n",
    "nn_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "\n",
    "We use the Wilcoxon test to compare the median values of these metrics between pairs of models. The Wilcoxon test is a non-parametric statistical test that assesses whether there is a statistically significant difference between two paired groups, for a fixed significance level. It is particularly suited for our use case, even with a large dataset. \n",
    "\n",
    "By performing this test on our model's performance metrics, we can make data-driven decisions about which models perform significantly better or worse for specific tasks. This analysis helps us choose the most suitable model for our problem and understand the statistical differences in performance.\n",
    "\n",
    "#### Why a Wilcoxon test?\n",
    "\n",
    "We chose this test considering the following:\n",
    "\n",
    "- **Non-parametric Test**: The Wilcoxon test is robust to deviations from normality, which is advantageous when dealing with a large dataset where the assumption of normal distribution might not hold.\n",
    "\n",
    "- **Paired Data**: We are comparing metrics from the same dataset, making it a paired comparison. This approach allows us to account for the specific characteristics of our data.\n",
    "\n",
    "- **Statistical Significance**: By calculating p-values using the Wilcoxon test, we can determine whether the observed differences are statistically significant, for a fixed significance level, which is crucial for meaningful model comparison.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "To achieve our goal, we defined a function designed to compare the performances using a Wilcoxon Hypothesis Test, organizing the results into a set of dataframes for easy interpretation. This function follows the following steps:\n",
    "\n",
    "1. Create dataframes for each metric such that each column holds the values for a model, and each row represents that model's value for that metric in a specific fold.\n",
    "\n",
    "2. For each of the dataframes created in step 1, perform Wilcoxon's Hypothesis Test to test if there is a difference in the median values of the folds for each model on that metric.\n",
    "\n",
    "3. Perform the test for each combination of two models and present the results in the form of (model 1, model 2, p-value) in a dataframe.\n",
    "\n",
    "4. Organize the p-values in a dictionary, with each metric name as the key and the associated dataframe as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_performance_comparison(scores_dict):\n",
    "\n",
    "    metrics_dfs_of_fold_per_model = {}\n",
    "    for model_name, model_metrics_folds_results in scores_dict.items():\n",
    "        for metric_name, metric_folds_results in model_metrics_folds_results.items():\n",
    "            if metric_name not in metrics_dfs_of_fold_per_model.keys():\n",
    "                metrics_dfs_of_fold_per_model[metric_name] = pd.DataFrame(\n",
    "                    index=[f\"fold{i}\" for i in range(1,len(metric_folds_results)+1)],\n",
    "                    data=metric_folds_results,\n",
    "                    columns=[model_name]\n",
    "                )\n",
    "            else: metrics_dfs_of_fold_per_model[metric_name][model_name] = metric_folds_results\n",
    "    \n",
    "    metrics_dfs_of_pvalues = {}\n",
    "    for metric_name, folds_models_values in metrics_dfs_of_fold_per_model.items():\n",
    "        pvalues_df_data = []\n",
    "        for i in range(len(folds_models_values.columns)):\n",
    "            for j in range(i+1, len(folds_models_values.columns)):\n",
    "                model1 = folds_models_values.columns[i]\n",
    "                model2 = folds_models_values.columns[j]\n",
    "                pvalue = ss.wilcoxon(folds_models_values[model1].to_numpy(), folds_models_values[model2].to_numpy()).pvalue\n",
    "                pvalues_df_data.append({\"model1\": model1, \"model2\": model2, \"pvalue\": pvalue})\n",
    "        metrics_dfs_of_pvalues[metric_name] = pd.DataFrame(data=pvalues_df_data, columns=[\"model1\", \"model2\", \"pvalue\"])\n",
    "\n",
    "    return metrics_dfs_of_pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give our function the scores obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {\n",
    "    'RF': rf_scores,\n",
    "    'XGB': xgb_scores,\n",
    "    'NN': nn_scores\n",
    "}\n",
    "metrics_dfs_of_fold_per_model = models_performance_comparison(scores_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Interpretation\n",
    "\n",
    "For the purposes of this project, we decided to consider a significance level of 0.05 (for a 95% confidence level).\n",
    "\n",
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dfs_of_fold_per_model['accuracy_score']\n",
    "# Write conclusions from the results in the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dfs_of_fold_per_model['f1_score']\n",
    "# Write conclusions from the results in the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dfs_of_fold_per_model['roc_auc_score']\n",
    "# Write conclusions from the results in the table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
